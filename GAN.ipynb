{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q09t-8dr374m"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "\n",
        "# Define generator model\n",
        "def build_generator(latent_dim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(4 * 4 * 256, input_dim=latent_dim))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Reshape((4, 4, 256)))\n",
        "    model.add(layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Define discriminator model\n",
        "def build_discriminator(input_shape):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(64, kernel_size=3, strides=2, padding='same', input_shape=input_shape))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.4))\n",
        "    model.add(layers.Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.4))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Define GAN model\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    model = models.Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model\n",
        "\n",
        "# Define constants\n",
        "latent_dim = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "# Build and compile discriminator\n",
        "discriminator = build_discriminator(input_shape)\n",
        "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Build generator\n",
        "generator = build_generator(latent_dim)\n",
        "\n",
        "# Build and compile GAN model\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "steps_per_epoch = x_train.shape[0] // batch_size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for step in range(steps_per_epoch):\n",
        "        # Sample random points in latent space\n",
        "        random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
        "\n",
        "        # Generate fake images using generator\n",
        "        generated_images = generator.predict(random_latent_vectors)\n",
        "\n",
        "        # Combine real and fake images into a batch for discriminator\n",
        "        real_images = x_train[np.random.randint(0, x_train.shape[0], batch_size)]\n",
        "        combined_images = np.concatenate([generated_images, real_images])\n",
        "\n",
        "        # Create labels for discriminator\n",
        "        labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
        "        labels += 0.05 * np.random.random(labels.shape)  # Add noise to labels\n",
        "\n",
        "        # Train discriminator\n",
        "        d_loss = discriminator.train_on_batch(combined_images, labels)\n",
        "\n",
        "        # Sample random points in latent space\n",
        "        random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
        "\n",
        "        # Create misleading labels for generator\n",
        "        misleading_targets = np.zeros((batch_size, 1))\n",
        "\n",
        "        # Train generator (via GAN model)\n",
        "        a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
        "\n",
        "        # Print progress\n",
        "        if step % 100 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Step {step+1}/{steps_per_epoch}, D Loss: {d_loss[0]}, G Loss: {a_loss}')\n",
        "\n",
        "    # Generate and save sample images after each epoch\n",
        "    if epoch % 1 == 0:\n",
        "        samples = 10  # Number of sample images to generate\n",
        "        latent_points = np.random.normal(size=(samples, latent_dim))\n",
        "        generated_images = generator.predict(latent_points) * 255  # Scale back to 0-255 range\n",
        "\n",
        "        # Plot generated images\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        for i in range(samples):\n",
        "            plt.subplot(1, samples, i+1)\n",
        "            plt.imshow(generated_images[i].astype('uint8'))  # Convert to uint8 for display\n",
        "            plt.axis('off')\n",
        "        plt.show()\n"
      ]
    }
  ]
}